{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3b6e2",
   "metadata": {},
   "source": [
    "### lodading datasets and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b12e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08529e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"google/boolq\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d9e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77b2baf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801cfdba166f49f495544e030ff3567f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_rows(batch, tokenizer):\n",
    "    tmp_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(batch['question'])):\n",
    "        concatenated =  batch['passage'][i] + '<|endoftext|>' +\\\n",
    "                        batch['question'][i] + '<|endoftext|>' +\\\n",
    "                        'Yes or No?<|endoftext|>'\n",
    "        tmp_list.append(concatenated)\n",
    "        \n",
    "        # Convert answer to label\n",
    "        answer = batch['answer'][i]\n",
    "        label = 1 if answer == True else 0\n",
    "        label_list.append(label)\n",
    "        \n",
    "    # Tokenize the concatenated text\n",
    "    tokenized = tokenizer(tmp_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    tokenized[\"labels\"] = torch.tensor(label_list)\n",
    "    return tokenized\n",
    "\n",
    "dataset_train = dataset['train'].map(\n",
    "    lambda batch: process_rows(batch, tokenizer), batched=True)\n",
    "dataset_validation = dataset['validation'].map(\n",
    "    lambda batch: process_rows(batch, tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236eadd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'passage', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9427\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce74d10",
   "metadata": {},
   "source": [
    "### loadding fundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e34e4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "609cdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2', \n",
    "        num_labels=2,\n",
    "        id2label={0: \"right\", 1: \"wrong\"},\n",
    "        label2id={\"wrong\": 0, \"right\": 1}\n",
    "        )\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5527ee10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17345ef",
   "metadata": {},
   "source": [
    "### evaluating original fundation model output\n",
    "\n",
    "Random pick some QA passages and check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e19f5da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b9fe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    per_device_eval_batch_size=5,\n",
    "    seed=42,\n",
    "    disable_tqdm=False,\n",
    "    \n",
    ")\n",
    "\n",
    "validation_sample = dataset_validation.select(range(0, 500))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset = validation_sample,\n",
    "    #eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30dcb1d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0964258909225464,\n",
       " 'eval_accuracy': 0.636,\n",
       " 'eval_runtime': 33.5153,\n",
       " 'eval_samples_per_second': 14.919,\n",
       " 'eval_steps_per_second': 2.984}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "#trainer.evaluate(eval_dataset=validation_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bdba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce6b962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(fan_in_fan_out = True, task_type=\"SEQ_CLS\")\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,984 || all params: 124,737,792 || trainable%: 0.23888830740245906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444d49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.from_numpy(predictions)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    loss = F.cross_entropy(predictions, labels)\n",
    "    accuracy = (torch.argmax(predictions, dim=1) == labels).float().mean()\n",
    "    \n",
    "    return {\"eval_loss\": loss.item(), \"eval_accuracy\": accuracy.item()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    learning_rate=1e-4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d73a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sample = dataset_validation.select(range(0, 500))\n",
    "trainer_sample = dataset_train.shuffle(seed=42).select(range(0, 5000))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainer_sample,\n",
    "    eval_dataset=validation_sample,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8186c3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1966373920440674,\n",
       " 'eval_accuracy': 0.36800000071525574,\n",
       " 'eval_runtime': 34.4256,\n",
       " 'eval_samples_per_second': 14.524,\n",
       " 'eval_steps_per_second': 2.905}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## evaluate before train\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddfa880f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 29:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.642331</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>0.642861</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.6653919372558593, metrics={'train_runtime': 1774.0147, 'train_samples_per_second': 5.637, 'train_steps_per_second': 1.127, 'total_flos': 4174360350796800.0, 'train_loss': 0.6653919372558593, 'epoch': 2.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60c80e",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc386758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora-tokenizer/tokenizer_config.json',\n",
       " 'lora-tokenizer/special_tokens_map.json',\n",
       " 'lora-tokenizer/vocab.json',\n",
       " 'lora-tokenizer/merges.txt',\n",
       " 'lora-tokenizer/added_tokens.json',\n",
       " 'lora-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"lora-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da047308",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"gpt2-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification, PeftConfig\n",
    "#config=PeftConfig(lora_model.config)\n",
    "lora_model_load = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2-lora\", ignore_mismatched_sizes=True, #config=lora_model.config,\n",
    ")\n",
    "\n",
    "lora_model_load.config.pad_token_id = tokenizer.eos_token_id\n",
    "#lora_model_load.config = lora_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14579e9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer_load = AutoTokenizer.from_pretrained(\"lora-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sample = dataset_validation.select(range(0, 500))\n",
    "#trainer_sample = dataset_validation.select(range(0, 3000))\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    #warmup_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model_load,\n",
    "    args=training_args,\n",
    "    #train_dataset=trainer_sample,\n",
    "    eval_dataset=validation_sample,\n",
    "    tokenizer=tokenizer_load,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_load),\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6423311233520508,\n",
       " 'eval_accuracy': 0.6460000276565552,\n",
       " 'eval_runtime': 33.4373,\n",
       " 'eval_samples_per_second': 14.953,\n",
       " 'eval_steps_per_second': 14.953}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Ranch is an American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott that debuted in 2016 on Netflix. The show takes place on the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado; detailing the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner. While the opening sequence shows scenes from Norwood and Ouray, Colorado and surrounding Ouray and San Miguel Counties, The Ranch is filmed on a sound stage in front of a live audience in Burbank, California. Each season consists of 20 episodes broken up into two parts, each containing 10 episodes.</td>\n",
       "      <td>is garrison from the ranch a real place</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lanugo (/ləˈnjuːɡoʊ/; from Latin lana ``wool'') is very thin, soft, usually unpigmented, downy hair that is sometimes found on the body of a fetal or new-born human. It is the first hair to be produced by the fetal hair follicles, and it usually appears around sixteen weeks of gestation and is abundant by week twenty. It is normally shed before birth, around seven or eight months of gestation, but is sometimes present at birth. It disappears on its own within a few weeks.</td>\n",
       "      <td>are babies in the womb covered in hair</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An administrative law judge (ALJ) in the United States is a judge and trier of fact who both presides over trials and adjudicates the claims or disputes (in other words, ALJ-controlled proceedings are bench trials) involving administrative law.</td>\n",
       "      <td>is an administrative law judge a real judge</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Plant research continued on the International Space Station. Biomass Production System was used on the ISS Expedition 4. The Vegetable Production System (Veggie) system was later used aboard ISS. Plants tested in Veggie before going into space included lettuce, Swiss chard, radishes, Chinese cabbage and peas. Red Romaine lettuce was grown in space on Expedition 40 which were harvested when mature, frozen and tested back on Earth. Expedition 44 members became the first American astronauts to eat plants grown in space on 10 August 2015, when their crop of Red Romaine was harvested. Since 2003 Russian cosmonauts have been eating half of their crop while the other half goes towards further research. In 2012, a sunflower bloomed aboard the ISS under the care of NASA astronaut Donald Pettit. In January 2016, US astronauts announced that a zinnia had blossomed aboard the ISS.</td>\n",
       "      <td>are there plants on the international space station</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HCF (The Hospitals Contribution Fund of Australia) was formed in 1932 to provide health insurance cover to Australians. Since then, it has grown to become one of the country's largest combined registered private health fund and life insurance organisations. HCF is the 3rd largest health insurance company by market share (10.3% in FY2010) and is the largest not-for-profit health fund in Australia.</td>\n",
       "      <td>is hcf a not for profit health fund</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bank and public holidays in Scotland are determined under the Banking and Financial Dealings Act 1971 and the St Andrew's Day Bank Holiday (Scotland) Act 2007. Unlike the rest of United Kingdom, most bank holidays are not recognised as statutory public holidays in Scotland, as most public holidays are determined by local authorities across Scotland. Some of these may be taken in lieu of statutory holidays, while others may be additional holidays, although many companies, including Royal Mail, do not follow all the holidays listed below; and many swap between English and local holidays. Many large shops and supermarkets continue to operate normally during public holidays, especially since there are no restrictions such as Sunday trading rules in Scotland.</td>\n",
       "      <td>does scotland have the same bank holidays as england</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nationals of any country may visit Montenegro without a visa for up to 30 days if they hold a passport with visas issued by Ireland, a Schengen Area member state, the United Kingdom or the United States or if they are permanent residents of those countries. Residents of the United Arab Emirates do not require a visa for up to 10 days, if they hold a return ticket and proof of accommodation.</td>\n",
       "      <td>can i go to montenegro with a schengen visa</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The previous major redesign of the iPhone, the 4.7-inch iPhone 6 and 5.5-inch iPhone 6 Plus, resulted in larger screen sizes. However a significant number of customers still preferred the 4-inch screen size of the iPhone 5 and 5S. Apple stated in their event that they sold 30 million 4-inch iPhones in 2015.</td>\n",
       "      <td>is the iphone se before the iphone 6</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trees have a wide variety of sizes and shapes and growth habits. Specimens may grow as individual trunks, multitrunk masses, coppices, clonal colonies, or even more exotic tree complexes. Most champion tree programs focus finding and measuring the largest single-trunk example of each species. There are three basic parameters commonly measured to characterize the size of a single trunk tree: height, girth, and crown spread. Additional details on the methodology of Tree height measurement, Tree girth measurement, Tree crown measurement, and Tree volume measurement are presented in the links herein. A detailed guideline to these basic measurements is provided in The Tree Measuring Guidelines of the Eastern Native Tree Society by Will Blozan.</td>\n",
       "      <td>can a tree have more than one trunk</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A SIM lock, simlock, network lock, carrier lock or (master) subsidy lock is a technical restriction built into GSM and CDMA mobile phones by mobile phone manufacturers for use by service providers to restrict the use of these phones to specific countries and/or networks. This is in contrast to a phone (retrospectively called SIM-free or unlocked) that does not impose any SIM restrictions.</td>\n",
       "      <td>does a sim free phone lock to a network</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             passage  \\\n",
       "0                                                                                                                                                     The Ranch is an American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott that debuted in 2016 on Netflix. The show takes place on the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado; detailing the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner. While the opening sequence shows scenes from Norwood and Ouray, Colorado and surrounding Ouray and San Miguel Counties, The Ranch is filmed on a sound stage in front of a live audience in Burbank, California. Each season consists of 20 episodes broken up into two parts, each containing 10 episodes.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                       Lanugo (/ləˈnjuːɡoʊ/; from Latin lana ``wool'') is very thin, soft, usually unpigmented, downy hair that is sometimes found on the body of a fetal or new-born human. It is the first hair to be produced by the fetal hair follicles, and it usually appears around sixteen weeks of gestation and is abundant by week twenty. It is normally shed before birth, around seven or eight months of gestation, but is sometimes present at birth. It disappears on its own within a few weeks.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               An administrative law judge (ALJ) in the United States is a judge and trier of fact who both presides over trials and adjudicates the claims or disputes (in other words, ALJ-controlled proceedings are bench trials) involving administrative law.   \n",
       "3  Plant research continued on the International Space Station. Biomass Production System was used on the ISS Expedition 4. The Vegetable Production System (Veggie) system was later used aboard ISS. Plants tested in Veggie before going into space included lettuce, Swiss chard, radishes, Chinese cabbage and peas. Red Romaine lettuce was grown in space on Expedition 40 which were harvested when mature, frozen and tested back on Earth. Expedition 44 members became the first American astronauts to eat plants grown in space on 10 August 2015, when their crop of Red Romaine was harvested. Since 2003 Russian cosmonauts have been eating half of their crop while the other half goes towards further research. In 2012, a sunflower bloomed aboard the ISS under the care of NASA astronaut Donald Pettit. In January 2016, US astronauts announced that a zinnia had blossomed aboard the ISS.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    HCF (The Hospitals Contribution Fund of Australia) was formed in 1932 to provide health insurance cover to Australians. Since then, it has grown to become one of the country's largest combined registered private health fund and life insurance organisations. HCF is the 3rd largest health insurance company by market share (10.3% in FY2010) and is the largest not-for-profit health fund in Australia.   \n",
       "5                                                                                                                       Bank and public holidays in Scotland are determined under the Banking and Financial Dealings Act 1971 and the St Andrew's Day Bank Holiday (Scotland) Act 2007. Unlike the rest of United Kingdom, most bank holidays are not recognised as statutory public holidays in Scotland, as most public holidays are determined by local authorities across Scotland. Some of these may be taken in lieu of statutory holidays, while others may be additional holidays, although many companies, including Royal Mail, do not follow all the holidays listed below; and many swap between English and local holidays. Many large shops and supermarkets continue to operate normally during public holidays, especially since there are no restrictions such as Sunday trading rules in Scotland.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Nationals of any country may visit Montenegro without a visa for up to 30 days if they hold a passport with visas issued by Ireland, a Schengen Area member state, the United Kingdom or the United States or if they are permanent residents of those countries. Residents of the United Arab Emirates do not require a visa for up to 10 days, if they hold a return ticket and proof of accommodation.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The previous major redesign of the iPhone, the 4.7-inch iPhone 6 and 5.5-inch iPhone 6 Plus, resulted in larger screen sizes. However a significant number of customers still preferred the 4-inch screen size of the iPhone 5 and 5S. Apple stated in their event that they sold 30 million 4-inch iPhones in 2015.   \n",
       "8                                                                                                                                       Trees have a wide variety of sizes and shapes and growth habits. Specimens may grow as individual trunks, multitrunk masses, coppices, clonal colonies, or even more exotic tree complexes. Most champion tree programs focus finding and measuring the largest single-trunk example of each species. There are three basic parameters commonly measured to characterize the size of a single trunk tree: height, girth, and crown spread. Additional details on the methodology of Tree height measurement, Tree girth measurement, Tree crown measurement, and Tree volume measurement are presented in the links herein. A detailed guideline to these basic measurements is provided in The Tree Measuring Guidelines of the Eastern Native Tree Society by Will Blozan.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            A SIM lock, simlock, network lock, carrier lock or (master) subsidy lock is a technical restriction built into GSM and CDMA mobile phones by mobile phone manufacturers for use by service providers to restrict the use of these phones to specific countries and/or networks. This is in contrast to a phone (retrospectively called SIM-free or unlocked) that does not impose any SIM restrictions.   \n",
       "\n",
       "                                               question  answer  predictions  \\\n",
       "0               is garrison from the ranch a real place   False            1   \n",
       "1                are babies in the womb covered in hair    True            1   \n",
       "2           is an administrative law judge a real judge    True            0   \n",
       "3   are there plants on the international space station    True            1   \n",
       "4                   is hcf a not for profit health fund    True            1   \n",
       "5  does scotland have the same bank holidays as england   False            1   \n",
       "6           can i go to montenegro with a schengen visa    True            1   \n",
       "7                  is the iphone se before the iphone 6   False            0   \n",
       "8                   can a tree have more than one trunk    True            1   \n",
       "9               does a sim free phone lock to a network   False            0   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       0  \n",
       "6       1  \n",
       "7       0  \n",
       "8       1  \n",
       "9       0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_for_manual_review = dataset_validation.shuffle(seed=42).select(range(0,10))\n",
    "\n",
    "results = trainer.predict(items_for_manual_review)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"passage\": [item[\"passage\"] for item in items_for_manual_review],\n",
    "        \"question\": [item[\"question\"] for item in items_for_manual_review],\n",
    "        \"answer\": [item[\"answer\"] for item in items_for_manual_review],\n",
    "        \"predictions\": results.predictions.argmax(axis=1),\n",
    "        \"labels\": results.label_ids,\n",
    "    }\n",
    ")\n",
    "# Show all the cell\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd7254",
   "metadata": {},
   "source": [
    "#### original fundation model eval_accuracy is 63.6%\n",
    "#### befor train LoRA model eval_accuracy is 36.8%\n",
    "#### After 2epcoch and 5000 dataset items to train, final model eval_accuracy is 65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11bcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
